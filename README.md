The HathiTrust is an online repository that provides page-level data of millions of books across non-fiction, novels, poetry and drama; Ted Underwood is the computational literary scholar who has published most extensively on what can be done with this material and my doctoral thesis drew from some summary files he and his collaborators made available online.

Out of a sense of completionism I want to see what claims about literary history can be verified by applying some machine learning models to the broader dataset.

Hathi iterates through every json, identifies how many pages of a given volume are written in English and writes the id to a txt file if this is > 70%. This takes a very long time to run, but is very much worthwhile, we get an attrition rate of just under 50%, I'll be putting the list of ids up on Google Drive once I'm further along in the project so others can benefit from it.

Metadata pulls the publication date, title and author from every English language json and writes them to a csv

A yet to be uploaded R file filter this enormous dataframe, removing all the fiction, poetry and drama entries that Underwood and his colleagues have provided us with and then a number of words  indicative of non-fictive texts. This is a brute-force method but non-fiction is by far the largest genre within the corpus and is therefore likely to skew any classifier. As well as that it's low hanging fruit; texts with the author 'United States' or 'Parliament' in the title are clearly municipal or governmental literature. [note to self: one neat verification method might be to look at Underwood's paper on this project, evaluate the % breakdowns and see if the fiction, poetry and drama ids we know of line up proportionate to the number of non-fictive texts we would expect to see before 1922, this could allow for their removal at one stroke]

In a separate context Underwood seems to have provided some interested parties with metadata pertaining to fictive texts published from 1945 to the present, which includes HathiTrust file ids. Because this is the part of my project I am most interested in investigating I have produced fiction post 45 which pulls in all the word frequency data from the relevant jsons, groups the words, their frequencies and assigns the result to year-relevant csvs. A soon to be uploaded R file will outline the normalisation + analysis and this will serve as the pilot for the project's final stage, which is to do much the same to every json in English, excluding those most obviously non-fictive texts and group every word frequency by word and then by year. Every year will then vote for the 1000 most frequent words by proportional representation.

Initially I planned to do this exclusively by unsupervised methods - and perhaps I still will just to see what will happen - but without Underwood's guiding documentation viz. what id goes with which genre I'll be expending a lot of time for less good results.
